{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists('./sparql'):\n",
    "    os.makedirs('./sparql/')\n",
    "#read\n",
    "df = pd.read_csv('data_formatted.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = [\n",
    "    ['id_prop_instance_of', 'instance of', 'multiple_items'],\n",
    "    ['id_prop_author',  'author', 'multiple_items'],\n",
    "    ['id_prop_editor', 'editor', 'multiple_items'],\n",
    "    ['id_prop_keyword', 'keyword', 'multiple_items'],\n",
    "    ['id_prop_publisher', 'publisher', 'item'],\n",
    "    ['id_prop_place_of_publication', 'place of publication', 'item'],\n",
    "    ['id_prop_publication_type', 'publication type', 'item'],\n",
    "    ['id_prop_journal', 'journal', 'item'],\n",
    "    ['id_prop_conference', 'conference', 'item'],\n",
    "    ['id_prop_series', 'series', 'item'],\n",
    "    ['id_prop_publication_date', 'publication date', 'string'],\n",
    "    ['id_prop_DOI', 'DOI', 'string'],\n",
    "    ['id_prop_issue', 'issue', 'string'],\n",
    "    ['id_prop_volume', 'volume', 'string'],\n",
    "    ['id_prop_pages', 'pages', 'string'],\n",
    "    ['id_prop_title', 'title', 'string'],\n",
    "    ['id_prop_reference_URL', 'reference URL', 'string']\n",
    "]\n",
    "\n",
    "props = pd.DataFrame(props, columns = ['id', 'label', 'data_type'])\n",
    "props.to_csv('./sparql/properties.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_items = [\n",
    "    ['id_base_work', 'Work'],\n",
    "    ['id_base_work_type', 'Work Type'],\n",
    "    ['id_base_creator', 'Creator'],\n",
    "    ['id_base_author', 'Author'],\n",
    "    ['id_base_editor', 'Editor'],\n",
    "    ['id_base_journal', 'Journal'],\n",
    "    ['id_base_publisher', 'Publisher'],\n",
    "    ['id_base_place_of_publication', 'Place of Publication'],\n",
    "    ['id_base_conference', 'Conference'],\n",
    "    ['id_base_series', 'Series'],\n",
    "    ['id_base_keyword', 'Keyword']\n",
    "]\n",
    "\n",
    "base_items = pd.DataFrame(base_items, columns = ['id', 'label'])\n",
    "base_items.to_csv('./sparql/items_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication Types (first level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['id', 'label', 'id_prop_instance_of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "publication_types = [\n",
    "    ['id_type_book', 'Book', 'id_base_work_type'],\n",
    "    ['id_type_newspaper', 'Newspaper Publication', 'id_base_work_type'],\n",
    "    ['id_type_article', 'Article', 'id_base_work_type'],\n",
    "    ['id_type_pik_series', 'PIK Series', 'id_base_work_type'],\n",
    "    ['id_type_report', 'Report', 'id_base_work_type'],\n",
    "    ['id_type_habilitation', 'Habilitation', 'id_base_work_type'],\n",
    "    ['id_type_diploma', 'Diploma', 'id_base_work_type'],\n",
    "    ['id_type_lecture', 'Lecture', 'id_base_work_type'],\n",
    "    ['id_type_conference_paper', 'Conference Paper', 'id_base_work_type'],\n",
    "    ['id_type_thesis', 'Thesis', 'id_base_work_type'],\n",
    "    ['id_type_software', 'Software Publication', 'id_base_work_type'],\n",
    "    ['id_type_data', 'Data Publication', 'id_base_work_type'],\n",
    "    ['id_type_epub', 'Electronic Publication', 'id_base_work_type'],\n",
    "    ['id_type_conference_proceedings', 'Conference Proceedings', 'id_base_work_type'],\n",
    "]\n",
    "\n",
    "\n",
    "pd.DataFrame(publication_types, columns=COLUMNS).to_csv('./sparql/items_1.csv', index=False)\n",
    "del publication_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication Types (second level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inherited_types = [\n",
    "    ['id_type_article_in_book', 'Article in Book', 'id_base_work_type;id_type_book;id_type_article'],\n",
    "    ['id_type_isi_article', 'ISI Article', 'id_base_work_type;id_type_article'],\n",
    "    ['id_type_other_article', 'Other Article', 'id_base_work_type;id_type_article'],\n",
    "    ['id_type_article_in_report', 'Article in Book', 'id_base_work_type;id_type_report;id_type_article'],\n",
    "    ['id_type_editied_book', 'Edited Book', 'id_base_work_type;id_type_book'],\n",
    "]\n",
    "\n",
    "pd.DataFrame(inherited_types, columns=COLUMNS).to_csv('./sparql/items_2.csv', index=False)\n",
    "del inherited_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan(x):\n",
    "    return (x is np.nan or x != x)\n",
    "\n",
    "def split_and_flatten(input_array, sep = ';', splitter = None, transform = str.strip):\n",
    "    array = []\n",
    "    if not splitter:\n",
    "        splitter = lambda items: [transform(item) for item in items.split(sep)]\n",
    "        \n",
    "    for items in input_array:\n",
    "        if not is_nan(items):\n",
    "            array.extend(splitter(items))\n",
    "    return array\n",
    "\n",
    "def get_id_map(arr, prefix = '_'):\n",
    "    dic = {};\n",
    "    counter = 0\n",
    "    for i, item in enumerate(arr):\n",
    "        dic[item] = 'id_'+ prefix + '_' + str(i)\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creators (Authors & Editors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = split_and_flatten(df['authors'].tolist())\n",
    "editors = split_and_flatten(df['editors'].tolist())\n",
    "creators = authors + editors\n",
    "\n",
    "authors_map = get_id_map(authors)\n",
    "editors_map = get_id_map(editors)\n",
    "creator_map = get_id_map(creators, 'creator')\n",
    "\n",
    "creators_data = []\n",
    "for name, _id in creator_map.items():\n",
    "    val = 'id_base_creator'\n",
    "    if name in authors_map:\n",
    "        val += ';id_base_author'\n",
    "    if name in editors_map:\n",
    "        val += ';id_base_editor'\n",
    "    creators_data.append([_id, name, val])\n",
    "\n",
    "data = pd.DataFrame(creators_data, columns=COLUMNS)\n",
    "\n",
    "#clean up namespace, we still need the creator_map later\n",
    "del authors, editors, creators, authors_map, editors_map, creators_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_keywords(string):\n",
    "    return [item.strip().lower() for item in re.split('(;|,)', string)]\n",
    "\n",
    "keywords = split_and_flatten(df['keywordsAndPeerReview'].tolist(), splitter=split_keywords)\n",
    "\n",
    "keywords_map = get_id_map(keywords, 'keyword')\n",
    "keyword_data = []\n",
    "for name, _id in keywords_map.items():\n",
    "    keyword_data.append([_id, name, 'id_base_keyword'])\n",
    "\n",
    "data = data.append(pd.DataFrame(keyword_data, columns=COLUMNS))\n",
    "\n",
    "del keywords, keyword_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_instance_of(series, parent, id_prefix):\n",
    "    series = series.dropna().unique().tolist()\n",
    "    items_map = get_id_map(series, id_prefix)\n",
    "    items_data = []\n",
    "    for name, _id in items_map.items():\n",
    "        items_data.append([_id, name, parent])\n",
    "    return (items_map, items_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_map, publisher_data = row_instance_of(df['publisher'], 'id_base_publisher', 'publisher')\n",
    "data = data.append(pd.DataFrame(publisher_data, columns=COLUMNS))\n",
    "del publisher_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_map, journal_data = row_instance_of(df['journal'], 'id_base_journal', 'journal')\n",
    "data = data.append(pd.DataFrame(journal_data, columns=COLUMNS))\n",
    "del journal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place of Publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_map, place_data = row_instance_of(df['place'], 'id_base_place_of_publication', 'place')\n",
    "data = data.append(pd.DataFrame(place_data, columns=COLUMNS))\n",
    "del place_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_map, conference_data = row_instance_of(df['conference'], 'id_base_conference', 'conference')\n",
    "data = data.append(pd.DataFrame(conference_data, columns=COLUMNS))\n",
    "del conference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_map, series_data = row_instance_of(df['series'], 'id_base_series', 'series')\n",
    "data = data.append(pd.DataFrame(series_data, columns=COLUMNS))\n",
    "del series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished chunk of data, write back\n",
    "data.to_csv('./sparql/items_3.csv', index=False)\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "publication_type_map = {\n",
    "    'inbook': 'id_type_article_in_book',\n",
    "    'confpaper':'id_type_conference_paper',\n",
    "    'lecture':'id_type_lecture',\n",
    "    'paperr':'id_type_isi_article',\n",
    "    'papern':'id_type_other_article',\n",
    "    'instseries':'id_type_pik_series',\n",
    "    'epup':'id_type_epub',\n",
    "    'book':'id_type_book',\n",
    "    'inreport':'id_type_article_in_report',\n",
    "    'report' :'id_type_report',\n",
    "    'edbook' : 'id_type_editied_book',\n",
    "    'thesis':'id_type_thesis',\n",
    "    'proceedings':'id_type_conference_proceedings',\n",
    "    'newspaper':'id_type_newspaper',\n",
    "    'dipl':'id_type_diploma', \n",
    "    'habil':'id_type_habilitation',\n",
    "    'data':'id_type_data',\n",
    "    'software':'id_type_software'\n",
    "}\n",
    "\n",
    "works = []\n",
    "# we need these maps for the following part\n",
    "# series_map, conference_map, place_map, journal_map, publisher_map, keywords_map, creator_map\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    work = {\n",
    "        'id': 'id_work_'+ str(row['id']),\n",
    "        'label': row['title'],\n",
    "        'id_prop_instance_of': 'id_base_work'\n",
    "    }\n",
    "    if not is_nan(row['type']):\n",
    "        work['id_prop_publication_type'] = publication_type_map[row['type']]\n",
    "        \n",
    "    if not is_nan(row['authors']):\n",
    "        authors = [name.strip() for name in row['authors'].split(';')]\n",
    "        authors = map(lambda name : creator_map[name], authors)\n",
    "        work['id_prop_author'] = ';'.join(authors)\n",
    "        \n",
    "    if not is_nan(row['editors']):\n",
    "        editors = [name.strip() for name in row['editors'].split(';')]\n",
    "        editors = map(lambda name : creator_map[name], editors)\n",
    "        work['id_prop_editor'] = ';'.join(editors)\n",
    "        \n",
    "        \n",
    "    if not is_nan(row['keywordsAndPeerReview']):\n",
    "        keywords = split_keywords(row['keywordsAndPeerReview'])\n",
    "        keywords = map(lambda name : keywords_map[name], keywords)\n",
    "        work['id_prop_keyword'] = ';'.join(keywords)\n",
    "            \n",
    "    if not is_nan(row['publisher']):\n",
    "        work['id_prop_publisher'] = publisher_map[row['publisher']]\n",
    "        \n",
    "        \n",
    "    if not is_nan(row['journal']):\n",
    "        work['id_prop_journal'] = journal_map[row['journal']]    \n",
    "    \n",
    "    if not is_nan(row['place']):\n",
    "        work['id_prop_place_of_publication'] = place_map[row['place']]\n",
    "        \n",
    "        \n",
    "    if not is_nan(row['conference']):\n",
    "        work['id_prop_conference'] = conference_map[row['conference']]\n",
    "    \n",
    "    \n",
    "    if not is_nan(row['series']):\n",
    "        work['id_prop_series'] = series_map[row['series']]\n",
    "        \n",
    "        \n",
    "    \n",
    "    if (not is_nan(row['startpage'])) and (not is_nan(row['endpage'])):\n",
    "        val = str(row['startpage']) + '-' + str(row['endpage'])\n",
    "        work['id_prop_pages'] = val\n",
    "    \n",
    "    work['id_prop_title'] = row['booktitle']\n",
    "    work['id_prop_issue'] = row['issue']\n",
    "    work['id_prop_volume'] = row['vol']\n",
    "    work['id_prop_publication_date'] = row['year']\n",
    "    work['id_prop_reference_URL'] = row['link']\n",
    "    \n",
    "    works.append(work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_data = pd.concat([pd.DataFrame(work, index=[0]) for work in works], ignore_index = True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_data.to_csv('./sparql/items_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- transitive properties?\n",
    "- what are the different data types? and how can we make it clear that this field should have this type of data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
