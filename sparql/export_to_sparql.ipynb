{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists('./sparql'):\n",
    "    os.makedirs('./sparql/')\n",
    "#read\n",
    "df = pd.read_csv('../pik_output.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_label(label):\n",
    "    if len(label) > 245:\n",
    "        return label[:245]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = [\n",
    "    ['id_prop_instance_of', 'instance of', 'wikibase-item'], #P31\n",
    "    ['id_prop_author',  'author', 'wikibase-item'], #P50\n",
    "    ['id_prop_editor', 'editor', 'wikibase-item'], #P98\n",
    "    ['id_prop_main_subject', 'main subject', 'wikibase-item'], #P921\n",
    "    ['id_prop_publisher', 'publisher', 'wikibase-item'], #P123\n",
    "    ['id_prop_place_of_publication', 'place of publication', 'wikibase-item'], #P291\n",
    "    ['id_prop_published_in', 'published in', 'wikibase-item'], #P1433 (for journals)\n",
    "    ['id_prop_part_of_the_series', 'part of the series', 'wikibase-item'], #P179 (for series)\n",
    "    ['id_prop_publication_date', 'publication date', 'string'], #P577\n",
    "    ['id_prop_DOI', 'DOI', 'string'], #P356\n",
    "    ['id_prop_issue', 'issue', 'string'], #P433\n",
    "    ['id_prop_volume', 'volume', 'string'], #P478\n",
    "    ['id_prop_number_of_pages', 'number of pages', 'string'], #P1104\n",
    "]\n",
    "\n",
    "props = pd.DataFrame(props, columns = ['id', 'label', 'data_type'])\n",
    "props['label'] = props['label'].map(filter_label)\n",
    "props.to_csv('./sparql/properties.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_items = [\n",
    "    # publication types\n",
    "    ['id_base_article', 'article'], # Q191067 (for paperr and papern)\n",
    "    ['id_base_chapter', 'chapter'], #Q1980247 (for inbook)\n",
    "    ['id_base_confpaper', 'conference paper'], #Q23927052\n",
    "    ['id_base_lecture', 'lecture'], #Q603773\n",
    "    ['id_base_report', 'report'], #Q10870555\n",
    "    ['id_base_epub', 'electronic publication'], #Q21572908\n",
    "    ['id_base_inreport', 'research report'], #Q59387148\n",
    "    ['id_base_intseries', 'technical report'], #Q3099732\n",
    "    ['id_base_book', 'book'], #Q571\n",
    "    ['id_base_newspaper', 'newspaper article'], #Q2495037\n",
    "    ['id_base_edbook', 'edited volume'], #Q1711593\n",
    "    ['id_base_data', 'data publication'], #Q17051824\n",
    "    ['id_base_software', 'software project'], #Q63437139\n",
    "    ['id_base_dipl', 'diploma thesis'], #Q30749496\n",
    "    ['id_base_habil', 'habilitation thesis'], #Q144362\n",
    "    ['id_base_thesis', 'doctoral thesis'], #Q187685\n",
    "    ['id_base_proceedings', 'proceedings'], #Q1143604\n",
    "    # the rest\n",
    "    ['id_base_author', 'author'], #Q482980\n",
    "    ['id_base_editor', 'editor'], #Q1607826\n",
    "    ['id_base_journal', 'academic journal'], #Q737498\n",
    "    ['id_base_publisher', 'publisher'], #Q2085381\n",
    "    ['id_base_place_of_publication', 'place of publication'], # does not exist in wikidata, we will use our own base class\n",
    "    ['id_base_series', 'series'], # does not exist in wikidata, we will use our own base class\n",
    "    ['id_base_main_subject', 'main subject'] # does not exist in wikidata, we will use our own base class\n",
    "]\n",
    "\n",
    "base_items = pd.DataFrame(base_items, columns = ['id', 'label'])\n",
    "base_items['label'] = base_items['label'].map(filter_label)\n",
    "base_items.to_csv('./sparql/items_0.csv', index=False),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication Types (first level)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "publication_types = [\n",
    "    ['id_type_book', 'Book', 'id_base_work_type'],\n",
    "    ['id_type_newspaper', 'Newspaper Publication', 'id_base_work_type'],\n",
    "    ['id_type_article', 'Article', 'id_base_work_type'],\n",
    "    ['id_type_pik_series', 'PIK Series', 'id_base_work_type'],\n",
    "    ['id_type_report', 'Report', 'id_base_work_type'],\n",
    "    ['id_type_habilitation', 'Habilitation', 'id_base_work_type'],\n",
    "    ['id_type_diploma', 'Diploma', 'id_base_work_type'],\n",
    "    ['id_type_lecture', 'Lecture', 'id_base_work_type'],\n",
    "    ['id_type_conference_paper', 'Conference Paper', 'id_base_work_type'],\n",
    "    ['id_type_thesis', 'Thesis', 'id_base_work_type'],\n",
    "    ['id_type_software', 'Software Publication', 'id_base_work_type'],\n",
    "    ['id_type_data', 'Data Publication', 'id_base_work_type'],\n",
    "    ['id_type_epub', 'Electronic Publication', 'id_base_work_type'],\n",
    "    ['id_type_editied_book', 'Edited Book', 'id_base_work_type'],\n",
    "    ['id_type_conference_proceedings', 'Conference Proceedings', 'id_base_work_type'],\n",
    "]\n",
    "\n",
    "\n",
    "pub = pd.DataFrame(publication_types, columns=COLUMNS)\n",
    "pub['label'] = pub['label'].map(filter_label)\n",
    "pub.to_csv('./sparql/items_1.csv', index=False)\n",
    "del publication_types, pub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication Types (second level)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inherited_types = [\n",
    "    ['id_type_article_in_book', 'Article in Book', 'id_base_work_type;id_type_book;id_type_article'],\n",
    "    ['id_type_isi_article', 'ISI Article', 'id_base_work_type;id_type_article'],\n",
    "    ['id_type_other_article', 'Other Article', 'id_base_work_type;id_type_article'],\n",
    "    ['id_type_article_in_report', 'Article in Book', 'id_base_work_type;id_type_report;id_type_article'],\n",
    "]\n",
    "\n",
    "pub = pd.DataFrame(inherited_types, columns=COLUMNS)\n",
    "pub['label'] = pub['label'].map(filter_label)\n",
    "pub.to_csv('./sparql/items_2.csv', index=False)\n",
    "del inherited_types, pub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan(x):\n",
    "    return (x is np.nan or x != x)\n",
    "\n",
    "def split_and_flatten(input_array, sep = ';', splitter = None, transform = str.strip):\n",
    "    array = []\n",
    "    if not splitter:\n",
    "        splitter = lambda items: [transform(item) for item in items.split(sep)]\n",
    "        \n",
    "    for items in input_array:\n",
    "        if not is_nan(items):\n",
    "            split = splitter(items)\n",
    "            not_empty = filter(lambda string: len(string) > 0, split)\n",
    "            array.extend(not_empty)\n",
    "    return list(set(array))\n",
    "\n",
    "def get_id_map(arr, prefix = '_'):\n",
    "    dic = {};\n",
    "    for i, item in enumerate(arr):\n",
    "        dic[item] = 'id_'+ prefix + '_' + str(i)\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creators (Authors & Editors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['id', 'label', 'id_prop_instance_of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = split_and_flatten(df['authors'].tolist())\n",
    "editors = split_and_flatten(df['editors'].tolist())\n",
    "creators = authors + editors\n",
    "\n",
    "authors_map = get_id_map(authors)\n",
    "editors_map = get_id_map(editors)\n",
    "creator_map = get_id_map(creators, 'creator')\n",
    "\n",
    "creators_data = []\n",
    "for name, _id in creator_map.items():\n",
    "    val = ''\n",
    "    if name in authors_map and name in editors_map:\n",
    "        val = 'id_base_author;id_base_editor'\n",
    "    elif name in editors_map:\n",
    "        val = 'id_base_editor'\n",
    "    elif name in authors_map:\n",
    "        val = 'id_base_author'\n",
    "    creators_data.append([_id, name, val])\n",
    "\n",
    "cr = pd.DataFrame(creators_data, columns=COLUMNS)\n",
    "cr['label'] = cr['label'].map(filter_label)\n",
    "cr.to_csv('./sparql/items_1.csv', index=False)\n",
    "\n",
    "#clean up namespace, we still need the creator_map later\n",
    "del authors, editors, creators, authors_map, editors_map, creators_data, cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_keywords(string):\n",
    "    values = [item.strip().lower() for item in re.split('(;|,)', string)]\n",
    "    return filter(lambda string: len(string) > 2, values)\n",
    "\n",
    "keywords = split_and_flatten(df['keywordsAndPeerReview'].tolist(), splitter=split_keywords)\n",
    "\n",
    "keywords_map = get_id_map(keywords, 'keyword')\n",
    "keyword_data = []\n",
    "for name, _id in keywords_map.items():\n",
    "    keyword_data.append([_id, name, 'id_base_main_subject'])\n",
    "\n",
    "data = pd.DataFrame(keyword_data, columns=COLUMNS)\n",
    "\n",
    "del keywords, keyword_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_instance_of(series, parent, id_prefix):\n",
    "    series = series.dropna().unique().tolist()\n",
    "    items_map = get_id_map(series, id_prefix)\n",
    "    items_data = []\n",
    "    for name, _id in items_map.items():\n",
    "        items_data.append([_id, name, parent])\n",
    "    return (items_map, items_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_map, publisher_data = row_instance_of(df['publisher'], 'id_base_publisher', 'publisher')\n",
    "data = data.append(pd.DataFrame(publisher_data, columns=COLUMNS))\n",
    "del publisher_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_map, journal_data = row_instance_of(df['journal'], 'id_base_journal', 'journal')\n",
    "data = data.append(pd.DataFrame(journal_data, columns=COLUMNS))\n",
    "del journal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place of Publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_map, place_data = row_instance_of(df['place'], 'id_base_place_of_publication', 'place')\n",
    "data = data.append(pd.DataFrame(place_data, columns=COLUMNS))\n",
    "del place_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conference"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ignored (for now at least)\n",
    "conference_map, conference_data = row_instance_of(df['conference'], 'id_base_conference', 'conference')\n",
    "data = data.append(pd.DataFrame(conference_data, columns=COLUMNS))\n",
    "del conference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_map, series_data = row_instance_of(df['Serie'], 'id_base_series', 'series')\n",
    "data = data.append(pd.DataFrame(series_data, columns=COLUMNS))\n",
    "del series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished chunk of data, write back\n",
    "data['label'] = data['label'].map(filter_label)\n",
    "data.to_csv('./sparql/items_2.csv', index=False)\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sixth chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "publication_type_map = {\n",
    "    'inbook': 'id_base_chapter',\n",
    "    'confpaper':'id_base_confpaper',\n",
    "    'lecture':'id_base_lecture',\n",
    "    'paperr':'id_base_article',\n",
    "    'papern':'id_base_article',\n",
    "    'instseries':'id_base_intseries',\n",
    "    'epup':'id_base_epub',\n",
    "    'book':'id_base_book',\n",
    "    'inreport':'id_base_inreport',\n",
    "    'report' :'id_base_report',\n",
    "    'edbook' : 'id_base_edbook',\n",
    "    'thesis':'id_base_thesis',\n",
    "    'proceedings':'id_base_proceedings',\n",
    "    'newspaper':'id_base_newspaper',\n",
    "    'dipl':'id_base_dipl', \n",
    "    'habil':'id_base_habil',\n",
    "    'data':'id_base_data',\n",
    "    'software':'id_base_software'\n",
    "}\n",
    "\n",
    "works = []\n",
    "# we need these maps for the following part\n",
    "# series_map, conference_map, place_map, journal_map, publisher_map, keywords_map, creator_map\n",
    "\n",
    "illegal = []\n",
    "for index, row in df.iterrows():\n",
    "    if is_nan(row['title']):\n",
    "        illegal.append(row)\n",
    "        continue\n",
    "    work = {\n",
    "        'id': 'id_work_'+ str(row['id']),\n",
    "        'label': row['title']\n",
    "    }\n",
    "    if not is_nan(row['type']):\n",
    "        work['id_prop_instance_of'] = publication_type_map[row['type']]\n",
    "        \n",
    "    if not is_nan(row['authors']):\n",
    "        authors = [name.strip() for name in row['authors'].split(';')]\n",
    "        authors = map(lambda name : creator_map[name], authors)\n",
    "        work['id_prop_author'] = ';'.join(authors)\n",
    "        \n",
    "    if not is_nan(row['editors']):\n",
    "        editors = [name.strip() for name in row['editors'].split(';')]\n",
    "        editors = map(lambda name : creator_map[name], editors)\n",
    "        work['id_prop_editor'] = ';'.join(editors)\n",
    "        \n",
    "        \n",
    "    if not is_nan(row['keywordsAndPeerReview']):\n",
    "        keywords = split_keywords(row['keywordsAndPeerReview'])\n",
    "        keywords = map(lambda name : keywords_map[name], keywords)\n",
    "        work['id_prop_main_subject'] = ';'.join(keywords)\n",
    "            \n",
    "    if not is_nan(row['publisher']):\n",
    "        work['id_prop_publisher'] = publisher_map[row['publisher']]\n",
    "        \n",
    "        \n",
    "    if not is_nan(row['journal']):\n",
    "        work['id_prop_published_in'] = journal_map[row['journal']]    \n",
    "    \n",
    "    if not is_nan(row['place']):\n",
    "        work['id_prop_place_of_publication'] = place_map[row['place']]\n",
    "        \n",
    "        \n",
    "#     if not is_nan(row['conference']):\n",
    "#         work['id_prop_conference'] = conference_map[row['conference']]\n",
    "    \n",
    "    if not is_nan(row['Serie']):\n",
    "        work['id_prop_part_of_the_series'] = series_map[row['Serie']]\n",
    "        \n",
    "        \n",
    "    \n",
    "    if (not is_nan(row['startpage'])) and (not is_nan(row['endpage'])):\n",
    "        val = str(row['startpage']) + '-' + str(row['endpage'])\n",
    "        work['id_prop_number_of_pages'] = val\n",
    "    \n",
    "#     work['id_prop_title'] = row['booktitle']\n",
    "    if not is_nan(row['issue']):\n",
    "        work['id_prop_issue'] = row['issue']\n",
    "        \n",
    "    if not is_nan(row['vol']):\n",
    "        work['id_prop_volume'] = row['vol']\n",
    "    \n",
    "    if not is_nan(row['year']):\n",
    "        work['id_prop_publication_date'] = str(int(row['year']))\n",
    "#     work['id_prop_reference_URL'] = row['link']\n",
    "    \n",
    "    works.append(work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_data = pd.concat([pd.DataFrame(work, index=[0]) for work in works], ignore_index = True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_data['label'] = works_data['label'].map(filter_label)\n",
    "works_data.to_csv('./sparql/items_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
